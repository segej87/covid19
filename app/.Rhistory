Recovered = col_double(),
Active = col_double(),
Lat = col_double(),
Long_ = col_double(),
Combined_Key = col_character()
)
col_names <- names(col_specs$cols)
path <- 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/'
data_file <- file.path('data', sprintf('%s.RData', strftime(max_date, '%m-%d-%Y')))
if (file.exists(data_file)) {
cat('Loading file\n')
load(data_file)
return(dat)
} else {
cat('Newer data detected - downloading')
dat <- tibble()
pb <- progress_bar$new(total = length(data_files))
for (d in data_files) {
csv_data <-tryCatch({
readr::read_csv(url(file.path(path, d)), col_types = col_specs, progress = TRUE)
}, warning = function(w) {
tryCatch({
readr::read_csv(url(file.path(path, d)), col_types = col_specs2, progress = TRUE)
}, warning = function(w) {
tmp <- suppressWarnings(readr::read_csv(url(file.path(path, d)),
col_types = col_specs, progress = TRUE))
rep_names <-setdiff(col_names, names(tmp))
tmp[rep_names] <- NA
tmp
})
}, error = function(e) {
if (grepl('unique', e)) {
cat(e)
}
}) %>%
rename_all(gsub, pattern = "[^[:alnum:]]", replacement = '.') %>%
rename_all(gsub, pattern = 'Lat$', replacement = 'Latitude') %>%
rename_all(gsub, pattern = 'Long_$', replacement = 'Longitude')
date = as.Date(basename(gsub('[.][[:alnum:]]+$', '', d)), '%m-%d-%Y')
dat <- dat %>%
bind_rows(csv_data %>%
mutate(load_date = date))
pb$tick()
}
dat$Country.Region <- sapply(dat$Country.Region, FUN = function(x) {ifelse(grepl('China', x), 'China', x)})
dat <- dat %>%
replace_na(list(Province.State = 'None', Country.Region = 'None')) %>%
mutate_if(is.character, as.factor)
if (!dir.exists('data')) dir.create('data')
save(dat, file = file.path('data', sprintf('%s.RData', strftime(max_date, '%m-%d-%Y'))))
return(dat)
}
}
assign(
'dat',
load_data() %>%
mutate_if(is.factor, as.character) %>%
mutate(Province.State = ifelse(Province.State == Country.Region,
'None',
Province.State)) %>%
mutate_if(is.character, as.factor),
envir = .GlobalEnv
)
assign(
'dat_summ',
dat %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE),
Deaths = sum(Deaths, na.rm = TRUE),
Recovered = sum(Recovered, na.rm = TRUE)) %>%
arrange(Country.Region, Province.State),
envir = .GlobalEnv
)
assign(
'world_base',
st_read('geo/Countries_WGS84.shp',
quiet = TRUE),
envir = .GlobalEnv
)
assign(
'lockdowns',
read.csv('data/lockdowns.csv') %>%
mutate(Lockdown.Date = as.Date(Lockdown.Date)),
envir = .GlobalEnv
)
assign(
'populations',
read.csv('data/populations.csv') %>%
replace_na(list(Province.State = 'None')) %>%
mutate(Country.Region = factor(Country.Region),
Province.State = factor(Province.State)),
envir = .GlobalEnv
)
assign(
'state_prov_grouped',
dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE),
Deaths = sum(Deaths, na.rm = TRUE),
Recovered = sum(Recovered, na.rm = TRUE)) %>%
group_by(Country.Region, Province.State) %>%
mutate(Confirmed_rate = Confirmed - lag(Confirmed, default = 0),
Deaths_rate = Deaths - lag(Deaths, default = 0),
Recovered_rate = Recovered - lag(Recovered, default = 0),
Confirmed_accel = Confirmed_rate - lag(Confirmed_rate, default = 0),
Deaths_accel = Deaths_rate - lag(Deaths_rate, default = 0),
Recovered_accel = Recovered_rate - lag(Recovered_rate, default = 0)) %>%
left_join(dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE)) %>%
filter(Confirmed >= 100) %>%
group_by(Country.Region, Province.State) %>%
summarise(First100Date = min(load_date, na.rm = TRUE)),
by = c('Country.Region', 'Province.State')) %>%
left_join(populations, by = c('Country.Region', 'Province.State')) %>%
replace_na(list(Population = 1)) %>%
mutate(normalized_date = as.numeric(difftime(load_date, First100Date, unit = 'days'))),
envir = .GlobalEnv
)
state_prov_grouped
length(which(is.na(state_prov_grouped$Population)))
state_prov_grouped$Population
assign(
'state_prov_grouped',
dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE),
Deaths = sum(Deaths, na.rm = TRUE),
Recovered = sum(Recovered, na.rm = TRUE)) %>%
group_by(Country.Region, Province.State) %>%
mutate(Confirmed_rate = Confirmed - lag(Confirmed, default = 0),
Deaths_rate = Deaths - lag(Deaths, default = 0),
Recovered_rate = Recovered - lag(Recovered, default = 0),
Confirmed_accel = Confirmed_rate - lag(Confirmed_rate, default = 0),
Deaths_accel = Deaths_rate - lag(Deaths_rate, default = 0),
Recovered_accel = Recovered_rate - lag(Recovered_rate, default = 0)) %>%
left_join(dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE)) %>%
filter(Confirmed >= 100) %>%
group_by(Country.Region, Province.State) %>%
summarise(First100Date = min(load_date, na.rm = TRUE)),
by = c('Country.Region', 'Province.State')) %>%
left_join(populations, by = c('Country.Region', 'Province.State')) %>%
# replace_na(list(Population = 1)) %>%
mutate(normalized_date = as.numeric(difftime(load_date, First100Date, unit = 'days'))),
envir = .GlobalEnv
)
length(which(is.na(state_prov_grouped$Populations)))
length(which(is.na(state_prov_grouped$Population)))
length(which(!is.na(state_prov_grouped$Population)))
test <- state_prov_grouped %>% filter(is.na(Population))
unique(test$Country.Region)
assign(
'populations',
read.csv('data/populations.csv') %>%
replace_na(list(Province.State = 'None')) %>%
mutate(Country.Region = factor(Country.Region),
Province.State = factor(Province.State)),
envir = .GlobalEnv
)
assign(
'state_prov_grouped',
dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE),
Deaths = sum(Deaths, na.rm = TRUE),
Recovered = sum(Recovered, na.rm = TRUE)) %>%
group_by(Country.Region, Province.State) %>%
mutate(Confirmed_rate = Confirmed - lag(Confirmed, default = 0),
Deaths_rate = Deaths - lag(Deaths, default = 0),
Recovered_rate = Recovered - lag(Recovered, default = 0),
Confirmed_accel = Confirmed_rate - lag(Confirmed_rate, default = 0),
Deaths_accel = Deaths_rate - lag(Deaths_rate, default = 0),
Recovered_accel = Recovered_rate - lag(Recovered_rate, default = 0)) %>%
left_join(dat_summ %>%
group_by(load_date, Country.Region, Province.State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE)) %>%
filter(Confirmed >= 100) %>%
group_by(Country.Region, Province.State) %>%
summarise(First100Date = min(load_date, na.rm = TRUE)),
by = c('Country.Region', 'Province.State')) %>%
left_join(populations, by = c('Country.Region', 'Province.State')) %>%
# replace_na(list(Population = 1)) %>%
mutate(normalized_date = as.numeric(difftime(load_date, First100Date, unit = 'days'))),
envir = .GlobalEnv
)
test <- state_prov_grouped %>% filter(is.na(Population))
unique(test$Country.Region)
test <- state_prov_grouped %>% filter(!is.na(Population))
unique(test$Country.Region)
china <- state_prov_grouped %>% filter(Country.Region == 'China')
china
us <- state_prov_grouped %>% filter(Country.Region == 'US', is.na(Population))
us
unique(us$Province.State)
install.packages("WDI")
library(WDI)
WDIsearch()
datasets <- WDIsearch()
head(datasets)
dataset$name
datasets$name
datasets[,2]
pop_datasets <- dataset_names %>%
filter(grepl('[Pp]op', dataset_names))
dataset_names <- datasets[,2]
pop_datasets <- dataset_names %>%
filter(grepl('[Pp]op', dataset_names))
pop_datasets <- dataset_names[which(grepl('[Pp]op', dataset_names))]
which(grepl('sp.pop', datasets[1,]))
pop_datasets <- dataset_names[which(grepl('[Ii]nd', dataset_names))]
pop_datasets
urb_dat <- WDI(indicator = 'SP.URB.TOTL.IN.ZS', start = 2019, end = 2019)
head(urb_dat)
urb_dat
urb_dat <- WDI(indicator = 'SP.URB.TOTL.IN.ZS', start = 2017, end = 2019)
urb_dat
urb_dat <- WDI(indicator = 'SP.URB.TOTL.IN.ZS', start = 2000, end = 2019)
names(urb_dat)
class(urb_dat$year)
country_urbs <- urb_dat %>%
group_by(country) %>%
filter(year == max(year)) %>%
summarise(Population = mean(SP.URB.TOTL.IN.ZS)) %>%
arrange(desc(Population)) %>%
rename(Province.State = 'None',
Country.Region = country)
country_urbs <- urb_dat %>%
mutate(Province.State = 'None') %>%
group_by(country) %>%
filter(year == max(year)) %>%
summarise(Population = mean(SP.URB.TOTL.IN.ZS)) %>%
arrange(desc(Population)) %>%
rename(Country.Region = country)
country_urbs
country_urbs <- urb_dat %>%
mutate(Province.State = 'None') %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Population = mean(SP.URB.TOTL.IN.ZS, na.rm = TRUE)) %>%
arrange(desc(Population)) %>%
rename(Country.Region = country)
country_urbs
length(which(is.na(country.urbs$Population)))
length(which(is.na(country_urbs$Population)))
length(which(!is.na(country_urbs$Population)))
country_urbs <- urb_dat %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Population = mean(SP.URB.TOTL.IN.ZS, na.rm = TRUE)) %>%
arrange(desc(Population)) %>%
rename(Country.Region = country)
names(urb_dat)
class(urb_dat$SP.URB.TOTL.IN.ZS)
hist(urb_dat$SP.URB.TOTL.IN.ZS)
country_urbs <- urb_dat %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = mean(SP.URB.TOTL.IN.ZS, na.rm = TRUE)) %>%
arrange(desc(Density)) %>%
mutate(Province.State = 'None') %>%
rename(Country.Region = country)
country_urbs
names(urb_dat)
max(urb_dat$year)
country_urbs <- urb_dat %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = mean(SP.URB.TOTL.IN.ZS, na.rm = TRUE)) %>%
arrange(desc(Density)) %>%
mutate(Province.State = 'None') %>%
rename(Country.Region = country)
country_urbs
country_urbs <- urb_dat %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = mean(SP.URB.TOTL.IN.ZS, na.rm = TRUE))
country_urbs
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS)
country_urbs
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = mean(Density, na.rm = TRUE))
country_urbs
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = max(Density, na.rm = TRUE))
country_urbs
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(year == max(year, na.rm = TRUE))
country_urbs
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(!is.na(Density)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = max(Density, na.rm = TRUE))
country_urbs
length(which(!is.na(country_urbs$Density)))
length(which(is.na(country_urbs$Density)))
pop_dat <- WDI(indicator = 'SP.POP.TOTL', start = 2000, end = 2019)
head(pop_dat)
library(lubridate)
year(Sys.Date())
urb_dat <- WDI(indicator = 'SP.URB.TOTL.IN.ZS', start = 2000, end = year(Sys.Date()))
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(!is.na(Density)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Density = max(Density, na.rm = TRUE))
pop_dat <- WDI(indicator = 'SP.POP.TOTL', start = 2000, end = year(Sys.Date()))
names(country_pop)
names(country_pops)
names(pop_dat)
country_pops <- pop_dat %>%
rename(Population = SP.POP.TOTL) %>%
group_by(country) %>%
filter(!is.na(Population)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Population = max(Population, na.rm = TRUE))
country_pops
country_urbs <- urb_dat %>%
rename(Density = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(!is.na(Urban.Pop.Perc.Total)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Urban.Pop.Perc.Total = max(Urban.Pop.Perc.Total, na.rm = TRUE))
country_urbs <- urb_dat %>%
rename(Urban.Pop.Perc.Total = SP.URB.TOTL.IN.ZS) %>%
group_by(country) %>%
filter(!is.na(Urban.Pop.Perc.Total)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Urban.Pop.Perc.Total = max(Urban.Pop.Perc.Total, na.rm = TRUE))
country_pops <- pop_dat %>%
rename(Population = SP.POP.TOTL) %>%
group_by(country) %>%
filter(!is.na(Population)) %>%
filter(year == max(year, na.rm = TRUE)) %>%
summarise(Population = max(Population, na.rm = TRUE))
joined <- country_pops %>%
left_join(country_urbs, by = 'country')
joined
write.csv(joined, 'data/wb_pop_urb.csv')
runApp()
shiny::runApp()
library(httr)
library(progress)
library(curl)
urlfile='https://api.github.com/repos/CSSEGISandData/COVID-19/git/trees/master?recursive=1'
load_data <- function() {
req <- tryCatch({
GET(urlfile)
}, error = function(e) {
'override'
})
if (length(req) == 1) {
if (req == 'override') {
warning('Could not connect to server - loading most recent data')
files <- list.files('data', pattern = '.RData')
files_as_dates <- as.Date(gsub('.Rdata', '', files), format = '%m-%d-%Y')
assign('max_data_date', max(files_as_dates), envir = .GlobalEnv)
assign('connected', FALSE, envir = .GlobalEnv)
load(file.path('data', files[which(files_as_dates == max(files_as_dates))]))
return(dat)
}
}
stop_for_status(req)
filelist <- unlist(lapply(content(req)$tree, "[", "path"), use.names = F)
# Get the path with daily reports
get_path_level <- function(x, ind) {
unlist(strsplit(paths_with_daily, split = '/'))[ind]
}
paths_with_daily <- grep('daily_reports', filelist, value = TRUE)
root_dir <- unique(sapply(paths_with_daily, FUN = get_path_level, ind = 1))
next_level <- unique(sapply(paths_with_daily, FUN = get_path_level, ind = 2))
# Get all data file names
data_files <- filelist[which(grepl(file.path(root_dir, next_level), filelist) &
!grepl('README|[.]gitignore', filelist) &
grepl('[.]csv', filelist))]
dates <- as.Date(gsub(paste0(file.path(root_dir, next_level), '|[/]|([.]csv)'), '', data_files), format = '%m-%d-%Y')
max_date <- max(dates)
assign('max_data_date', max_date, envir = .GlobalEnv)
assign('connected', TRUE, envir = .GlobalEnv)
# Define column specifications
col_specs <- cols(
FIPS = col_integer(),
Admin2 = col_character(),
`Province/State` = col_character(),
`Country/Region` = col_character(),
`Last Update` = col_datetime(format = ""),
Confirmed = col_double(),
Deaths = col_double(),
Recovered = col_double(),
Active = col_double(),
Latitude = col_double(),
Longitude = col_double(),
Combined_Key = col_character()
)
col_specs2 <- cols(
FIPS = col_integer(),
Admin2 = col_character(),
Province_State = col_character(),
Country_Region = col_character(),
Last_Update = col_datetime(format = ""),
Confirmed = col_double(),
Deaths = col_double(),
Recovered = col_double(),
Active = col_double(),
Lat = col_double(),
Long_ = col_double(),
Combined_Key = col_character()
)
col_names <- names(col_specs$cols)
path <- 'https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/'
data_file <- file.path('data', sprintf('%s.RData', strftime(max_date, '%m-%d-%Y')))
if (file.exists(data_file)) {
cat('Loading file\n')
load(data_file)
return(dat)
} else {
cat('Newer data detected - downloading')
dat <- tibble()
pb <- progress_bar$new(total = length(data_files))
for (d in data_files) {
csv_data <-tryCatch({
readr::read_csv(url(file.path(path, d)), col_types = col_specs, progress = TRUE)
}, warning = function(w) {
tryCatch({
readr::read_csv(url(file.path(path, d)), col_types = col_specs2, progress = TRUE)
}, warning = function(w) {
tmp <- suppressWarnings(readr::read_csv(url(file.path(path, d)),
col_types = col_specs, progress = TRUE))
rep_names <-setdiff(col_names, names(tmp))
tmp[rep_names] <- NA
tmp
})
}, error = function(e) {
if (grepl('unique', e)) {
cat(e)
}
}) %>%
rename_all(gsub, pattern = "[^[:alnum:]]", replacement = '.') %>%
rename_all(gsub, pattern = 'Lat$', replacement = 'Latitude') %>%
rename_all(gsub, pattern = 'Long_$', replacement = 'Longitude')
date = as.Date(basename(gsub('[.][[:alnum:]]+$', '', d)), '%m-%d-%Y')
dat <- dat %>%
bind_rows(csv_data %>%
mutate(load_date = date))
pb$tick()
}
dat$Country.Region <- sapply(dat$Country.Region, FUN = function(x) {ifelse(grepl('China', x), 'China', x)})
dat <- dat %>%
replace_na(list(Province.State = 'None', Country.Region = 'None')) %>%
mutate_if(is.character, as.factor)
if (!dir.exists('data')) dir.create('data')
save(dat, file = file.path('data', sprintf('%s.RData', strftime(max_date, '%m-%d-%Y'))))
return(dat)
}
}
assign(
'world_base',
st_read('geo/Countries_WGS84.shp',
quiet = TRUE),
envir = .GlobalEnv
)
assign(
'lockdowns',
read.csv('data/lockdowns.csv') %>%
mutate(Lockdown.Date = as.Date(Lockdown.Date)),
envir = .GlobalEnv
)
assign(
'populations',
read.csv('data/populations.csv') %>%
replace_na(list(Province.State = 'None')) %>%
mutate(Country.Region = factor(Country.Region),
Province.State = factor(Province.State)),
envir = .GlobalEnv
)
assign(
'dat',
load_data() %>%
mutate_if(is.factor, as.character) %>%
mutate(Province.State = ifelse(Province.State == Country.Region,
'None',
Province.State)) %>%
mutate_if(is.character, as.factor) %>%
left_join(populations, by = c('Province.State', 'Country.Region')),
envir = .GlobalEnv
)
names(dat)
dat %>% filter(Country.Region == 'China')
dat %>% filter(Country.Region == 'China') %>% select(Population)
assign(
'state_populations',
read.csv('data/state_pops.csv') %>%
replace_na(list(Province.State = 'None')) %>%
mutate(Country.Region = factor(Country.Region),
Province.State = factor(Province.State)),
envir = .GlobalEnv
)
state_populations
runApp()
